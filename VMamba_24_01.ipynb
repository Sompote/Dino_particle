{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv5dU5kj1MTB",
        "outputId": "5c380842-0df9-4769-b29c-5b5a080c7469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.12) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs) (6.0.1)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n",
            "Collecting submitit\n",
            "  Downloading submitit-1.5.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX, submitit\n",
            "Successfully installed submitit-1.5.1 tensorboardX-2.6.2.2\n",
            "Collecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (3.27.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (3.13.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (2.1.0+cu121)\n",
            "Collecting lit (from triton==2.0.0)\n",
            "  Downloading lit-17.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (2023.6.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-curand-cu12/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting nvidia-curand-cu12==10.3.2.106 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (0.42.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->triton==2.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->triton==2.0.0) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=3d0e93689114f671f38ea353e44a69a4605e142f5be40f4f8007c98942833b80\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, triton\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-17.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
            "Collecting causal_conv1d==1.0.0\n",
            "  Downloading causal_conv1d-1.0.0.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal_conv1d==1.0.0) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal_conv1d==1.0.0) (23.2)\n",
            "Collecting ninja (from causal_conv1d==1.0.0)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->causal_conv1d==1.0.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->causal_conv1d==1.0.0) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->causal_conv1d==1.0.0) (3.27.9)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->causal_conv1d==1.0.0) (17.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->causal_conv1d==1.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->causal_conv1d==1.0.0) (1.3.0)\n",
            "Building wheels for collected packages: causal_conv1d\n",
            "  Building wheel for causal_conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal_conv1d: filename=causal_conv1d-1.0.0-cp310-cp310-linux_x86_64.whl size=9145277 sha256=476b13556bd7068d8d0ca49a5073677b8ee3c5f7a92d6be3159ff8b3b9fd063e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/48/f5/eb0c6d6d8e00131eaa57927b537a23832b37e2f01b801d9c5d\n",
            "Successfully built causal_conv1d\n",
            "Installing collected packages: ninja, causal_conv1d\n",
            "Successfully installed causal_conv1d-1.0.0 ninja-1.11.1.1\n",
            "Collecting mamba_ssm==1.0.1\n",
            "  Downloading mamba_ssm-1.0.1.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1) (23.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1) (1.11.1.1)\n",
            "Collecting einops (from mamba_ssm==1.0.1)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1) (2.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1) (4.35.2)\n",
            "Requirement already satisfied: causal_conv1d in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1) (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==1.0.1) (11.7.91)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton->mamba_ssm==1.0.1) (3.27.9)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton->mamba_ssm==1.0.1) (17.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->mamba_ssm==1.0.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->mamba_ssm==1.0.1) (0.42.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->mamba_ssm==1.0.1) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba_ssm==1.0.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==1.0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==1.0.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==1.0.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==1.0.1) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba_ssm==1.0.1) (1.3.0)\n",
            "Building wheels for collected packages: mamba_ssm\n",
            "  Building wheel for mamba_ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba_ssm: filename=mamba_ssm-1.0.1-cp310-cp310-linux_x86_64.whl size=146630099 sha256=b2131da71619353377bee35d0e2359f7fc82d71fdf91ab8fb487f56d27152182\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/cf/65/cc589985f9689241fe2c154ce1c60738f58a24e76ce474cc20\n",
            "Successfully built mamba_ssm\n",
            "Installing collected packages: einops, mamba_ssm\n",
            "Successfully installed einops-0.7.0 mamba_ssm-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip -V\n",
        "#!pip install torch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "# We use py110 cu117 torch113\n",
        "!pip install packaging\n",
        "!pip install timm==0.4.12\n",
        "!pip install pytest chardet yacs termcolor\n",
        "!pip install submitit tensorboardX\n",
        "!pip install triton==2.0.0\n",
        "!pip install causal_conv1d==1.0.0  # causal_conv1d-1.0.0+cu118torch1.13cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
        "!pip install mamba_ssm==1.0.1  # mamba_ssm-1.0.1+cu118torch1.13cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# Swin Transformer\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Written by Ze Liu\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "\n",
        "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
        "from timm.utils import accuracy, AverageMeter\n",
        "\n",
        "#from utils.config import get_config\n",
        "from models import build_model\n",
        "from data import build_loader\n",
        "from utils.lr_scheduler import build_scheduler\n",
        "from utils.optimizer import build_optimizer\n",
        "from utils.logger import create_logger\n",
        "from utils.utils import load_checkpoint, load_pretrained, save_checkpoint, NativeScalerWithGradNormCount, auto_resume_helper, \\\n",
        "    reduce_tensor\n",
        "\n",
        "from timm.utils import ModelEma as ModelEma\n",
        "from utils.utils_ema import load_checkpoint_ema, load_pretrained_ema, save_checkpoint_ema\n",
        "print(f\"================= {torch.multiprocessing.get_start_method()} ==================\")\n",
        "torch.multiprocessing.set_start_method(\"spawn\", force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "aiUOoPAp1PVV",
        "outputId": "59bad44a-745a-40ad-b1d5-b87bd6e10307"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-008e0371a608>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#from utils.config import get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "from functools import partial\n",
        "from typing import Optional, Callable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from einops import rearrange, repeat\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "try:\n",
        "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# an alternative for mamba_ssm (in which causal_conv1d is needed)\n",
        "try:\n",
        "    from selective_scan import selective_scan_fn as selective_scan_fn_v1\n",
        "    from selective_scan import selective_scan_ref as selective_scan_ref_v1\n",
        "except:\n",
        "    pass\n",
        "\n",
        "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
        "\n",
        "\n",
        "def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):\n",
        "    \"\"\"\n",
        "    u: r(B D L)\n",
        "    delta: r(B D L)\n",
        "    A: r(D N)\n",
        "    B: r(B N L)\n",
        "    C: r(B N L)\n",
        "    D: r(D)\n",
        "    z: r(B D L)\n",
        "    delta_bias: r(D), fp32\n",
        "\n",
        "    ignores:\n",
        "        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # fvcore.nn.jit_handles\n",
        "    def get_flops_einsum(input_shapes, equation):\n",
        "        np_arrs = [np.zeros(s) for s in input_shapes]\n",
        "        optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n",
        "        for line in optim.split(\"\\n\"):\n",
        "            if \"optimized flop\" in line.lower():\n",
        "                # divided by 2 because we count MAC (multiply-add counted as one flop)\n",
        "                flop = float(np.floor(float(line.split(\":\")[-1]) / 2))\n",
        "                return flop\n",
        "\n",
        "\n",
        "    assert not with_complex\n",
        "\n",
        "    flops = 0 # below code flops = 0\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        dtype_in = u.dtype\n",
        "        u = u.float()\n",
        "        delta = delta.float()\n",
        "        if delta_bias is not None:\n",
        "            delta = delta + delta_bias[..., None].float()\n",
        "        if delta_softplus:\n",
        "            delta = F.softplus(delta)\n",
        "        batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
        "        is_variable_B = B.dim() >= 3\n",
        "        is_variable_C = C.dim() >= 3\n",
        "        if A.is_complex():\n",
        "            if is_variable_B:\n",
        "                B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
        "            if is_variable_C:\n",
        "                C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
        "        else:\n",
        "            B = B.float()\n",
        "            C = C.float()\n",
        "        x = A.new_zeros((batch, dim, dstate))\n",
        "        ys = []\n",
        "        \"\"\"\n",
        "\n",
        "    flops += get_flops_einsum([[B, D, L], [D, N]], \"bdl,dn->bdln\")\n",
        "    if with_Group:\n",
        "        flops += get_flops_einsum([[B, D, L], [B, N, L], [B, D, L]], \"bdl,bnl,bdl->bdln\")\n",
        "    else:\n",
        "        flops += get_flops_einsum([[B, D, L], [B, D, N, L], [B, D, L]], \"bdl,bdnl,bdl->bdln\")\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
        "        if not is_variable_B:\n",
        "            deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
        "        else:\n",
        "            if B.dim() == 3:\n",
        "                deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
        "            else:\n",
        "                B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
        "                deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
        "        if is_variable_C and C.dim() == 4:\n",
        "            C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
        "        last_state = None\n",
        "        \"\"\"\n",
        "\n",
        "    in_for_flops = B * D * N\n",
        "    if with_Group:\n",
        "        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], \"bdn,bdn->bd\")\n",
        "    else:\n",
        "        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], \"bdn,bn->bd\")\n",
        "    flops += L * in_for_flops\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        for i in range(u.shape[2]):\n",
        "            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
        "            if not is_variable_C:\n",
        "                y = torch.einsum('bdn,dn->bd', x, C)\n",
        "            else:\n",
        "                if C.dim() == 3:\n",
        "                    y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
        "                else:\n",
        "                    y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
        "            if i == u.shape[2] - 1:\n",
        "                last_state = x\n",
        "            if y.is_complex():\n",
        "                y = y.real * 2\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=2) # (batch dim L)\n",
        "        \"\"\"\n",
        "\n",
        "    if with_D:\n",
        "        flops += B * D * L\n",
        "    if with_Z:\n",
        "        flops += B * D * L\n",
        "    if False:\n",
        "        ...\n",
        "        \"\"\"\n",
        "        out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
        "        if z is not None:\n",
        "            out = out * F.silu(z)\n",
        "        out = out.to(dtype=dtype_in)\n",
        "        \"\"\"\n",
        "\n",
        "    return flops\n",
        "\n",
        "\n",
        "class PatchEmbed2D(nn.Module):\n",
        "    r\"\"\" Image to Patch Embedding\n",
        "    Args:\n",
        "        patch_size (int): Patch token size. Default: 4.\n",
        "        in_chans (int): Number of input image channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):\n",
        "        super().__init__()\n",
        "        if isinstance(patch_size, int):\n",
        "            patch_size = (patch_size, patch_size)\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).permute(0, 2, 3, 1)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging2D(nn.Module):\n",
        "    r\"\"\" Patch Merging Layer.\n",
        "    Args:\n",
        "        input_resolution (tuple[int]): Resolution of input feature.\n",
        "        dim (int): Number of input channels.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        SHAPE_FIX = [-1, -1]\n",
        "        if (W % 2 != 0) or (H % 2 != 0):\n",
        "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
        "            SHAPE_FIX[0] = H // 2\n",
        "            SHAPE_FIX[1] = W // 2\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
        "\n",
        "        if SHAPE_FIX[0] > 0:\n",
        "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
        "\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
        "        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SS2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        d_state=16,\n",
        "        # d_state=\"auto\", # 20240109\n",
        "        d_conv=3,\n",
        "        expand=2,\n",
        "        dt_rank=\"auto\",\n",
        "        dt_min=0.001,\n",
        "        dt_max=0.1,\n",
        "        dt_init=\"random\",\n",
        "        dt_scale=1.0,\n",
        "        dt_init_floor=1e-4,\n",
        "        dropout=0.,\n",
        "        conv_bias=True,\n",
        "        bias=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        # self.d_state = math.ceil(self.d_model / 6) if d_state == \"auto\" else d_model # 20240109\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
        "        self.conv2d = nn.Conv2d(\n",
        "            in_channels=self.d_inner,\n",
        "            out_channels=self.d_inner,\n",
        "            groups=self.d_inner,\n",
        "            bias=conv_bias,\n",
        "            kernel_size=d_conv,\n",
        "            padding=(d_conv - 1) // 2,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.x_proj = (\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
        "        )\n",
        "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)\n",
        "        del self.x_proj\n",
        "\n",
        "        self.dt_projs = (\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
        "        )\n",
        "        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)\n",
        "        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)\n",
        "        del self.dt_projs\n",
        "\n",
        "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
        "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
        "\n",
        "        # self.selective_scan = selective_scan_fn\n",
        "        self.forward_core = self.forward_corev0\n",
        "\n",
        "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
        "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
        "\n",
        "    @staticmethod\n",
        "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):\n",
        "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
        "\n",
        "        # Initialize special dt projection to preserve variance at initialization\n",
        "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
        "        if dt_init == \"constant\":\n",
        "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
        "        elif dt_init == \"random\":\n",
        "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
        "        dt = torch.exp(\n",
        "            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
        "            + math.log(dt_min)\n",
        "        ).clamp(min=dt_init_floor)\n",
        "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
        "        with torch.no_grad():\n",
        "            dt_proj.bias.copy_(inv_dt)\n",
        "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
        "        dt_proj.bias._no_reinit = True\n",
        "\n",
        "        return dt_proj\n",
        "\n",
        "    @staticmethod\n",
        "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
        "        # S4D real initialization\n",
        "        A = repeat(\n",
        "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
        "            \"n -> d n\",\n",
        "            d=d_inner,\n",
        "        ).contiguous()\n",
        "        A_log = torch.log(A)  # Keep A_log in fp32\n",
        "        if copies > 1:\n",
        "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
        "            if merge:\n",
        "                A_log = A_log.flatten(0, 1)\n",
        "        A_log = nn.Parameter(A_log)\n",
        "        A_log._no_weight_decay = True\n",
        "        return A_log\n",
        "\n",
        "    @staticmethod\n",
        "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
        "        # D \"skip\" parameter\n",
        "        D = torch.ones(d_inner, device=device)\n",
        "        if copies > 1:\n",
        "            D = repeat(D, \"n1 -> r n1\", r=copies)\n",
        "            if merge:\n",
        "                D = D.flatten(0, 1)\n",
        "        D = nn.Parameter(D)  # Keep in fp32\n",
        "        D._no_weight_decay = True\n",
        "        return D\n",
        "\n",
        "    def forward_corev0(self, x: torch.Tensor):\n",
        "        self.selective_scan = selective_scan_fn\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        L = H * W\n",
        "        K = 4\n",
        "\n",
        "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
        "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
        "\n",
        "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
        "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
        "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
        "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
        "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
        "\n",
        "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
        "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
        "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Ds = self.Ds.float().view(-1) # (k * d)\n",
        "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
        "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
        "\n",
        "        out_y = self.selective_scan(\n",
        "            xs, dts,\n",
        "            As, Bs, Cs, Ds, z=None,\n",
        "            delta_bias=dt_projs_bias,\n",
        "            delta_softplus=True,\n",
        "            return_last_state=False,\n",
        "        ).view(B, K, -1, L)\n",
        "        assert out_y.dtype == torch.float\n",
        "\n",
        "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
        "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "\n",
        "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
        "\n",
        "    # an alternative to forward_corev1\n",
        "    def forward_corev1(self, x: torch.Tensor):\n",
        "        self.selective_scan = selective_scan_fn_v1\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        L = H * W\n",
        "        K = 4\n",
        "\n",
        "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
        "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
        "\n",
        "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
        "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
        "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
        "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
        "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
        "\n",
        "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
        "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
        "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
        "        Ds = self.Ds.float().view(-1) # (k * d)\n",
        "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
        "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
        "\n",
        "        out_y = self.selective_scan(\n",
        "            xs, dts,\n",
        "            As, Bs, Cs, Ds,\n",
        "            delta_bias=dt_projs_bias,\n",
        "            delta_softplus=True,\n",
        "        ).view(B, K, -1, L)\n",
        "        assert out_y.dtype == torch.float\n",
        "\n",
        "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
        "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
        "\n",
        "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
        "\n",
        "    def forward(self, x: torch.Tensor, **kwargs):\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        xz = self.in_proj(x)\n",
        "        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)\n",
        "\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.act(self.conv2d(x)) # (b, d, h, w)\n",
        "        y1, y2, y3, y4 = self.forward_core(x)\n",
        "        assert y1.dtype == torch.float32\n",
        "        y = y1 + y2 + y3 + y4\n",
        "        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n",
        "        y = self.out_norm(y)\n",
        "        y = y * F.silu(z)\n",
        "        out = self.out_proj(y)\n",
        "        if self.dropout is not None:\n",
        "            out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class VSSBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int = 0,\n",
        "        drop_path: float = 0,\n",
        "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "        attn_drop_rate: float = 0,\n",
        "        d_state: int = 16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ln_1 = norm_layer(hidden_dim)\n",
        "        self.self_attention = SS2D(d_model=hidden_dim, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
        "        self.drop_path = DropPath(drop_path)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        x = input + self.drop_path(self.self_attention(self.ln_1(input)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class VSSLayer(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        depth (int): Number of blocks.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        attn_drop=0.,\n",
        "        drop_path=0.,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        downsample=None,\n",
        "        use_checkpoint=False,\n",
        "        d_state=16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            VSSBlock(\n",
        "                hidden_dim=dim,\n",
        "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                norm_layer=norm_layer,\n",
        "                attn_drop_rate=attn_drop,\n",
        "                d_state=d_state,\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "\n",
        "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
        "            def _init_weights(module: nn.Module):\n",
        "                for name, p in module.named_parameters():\n",
        "                    if name in [\"out_proj.weight\"]:\n",
        "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "            self.apply(_init_weights)\n",
        "\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VSSM(nn.Module):\n",
        "    def __init__(self, patch_size=4, in_chans=3, num_classes=1000, depths=[2, 2, 9, 2],\n",
        "                 dims=[96, 192, 384, 768], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        if isinstance(dims, int):\n",
        "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
        "        self.embed_dim = dims[0]\n",
        "        self.num_features = dims[-1]\n",
        "        self.dims = dims\n",
        "\n",
        "        self.patch_embed = PatchEmbed2D(patch_size=patch_size, in_chans=in_chans, embed_dim=self.embed_dim,\n",
        "            norm_layer=norm_layer if patch_norm else None)\n",
        "\n",
        "        # WASTED absolute position embedding ======================\n",
        "        self.ape = False\n",
        "        # self.ape = False\n",
        "        # drop_rate = 0.0\n",
        "        if self.ape:\n",
        "            self.patches_resolution = self.patch_embed.patches_resolution\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, *self.patches_resolution, self.embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = VSSLayer(\n",
        "                dim=dims[i_layer],\n",
        "                depth=depths[i_layer],\n",
        "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=PatchMerging2D if (i_layer < self.num_layers - 1) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m: nn.Module):\n",
        "        \"\"\"\n",
        "        out_proj.weight which is previously initilized in VSSBlock, would be cleared in nn.Linear\n",
        "        no fc.weight found in the any of the model parameters\n",
        "        no nn.Embedding found in the any of the model parameters\n",
        "        so the thing is, VSSBlock initialization is useless\n",
        "\n",
        "        Conv2D is not intialized !!!\n",
        "        \"\"\"\n",
        "        # print(m, getattr(getattr(m, \"weight\", nn.Identity()), \"INIT\", None), isinstance(m, nn.Linear), \"======================\")\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = torch.flatten(x, 1, 2) # B H W C -> B L C\n",
        "        x = self.norm(x)  # B L C\n",
        "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "8siAngBF2hFE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFkJsXpkBiqJ",
        "outputId": "76acafec-0956-471a-f8fa-58678217a177"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, ViTModel,ViTFeatureExtractor,AutoModel\n",
        "from transformers import Dinov2Config, Dinov2Model\n",
        "\n",
        "\n",
        "torch.set_warn_always(False)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "config=Dinov2Config(image_size=1800)\n",
        "model_di=Dinov2Model(config)\n",
        "\n",
        "#model_di = Dinov2Model.from_pretrained(\"facebook/dinov2-large\")\n",
        "\n",
        "#model_di = AutoModel.from_pretrained('facebook/dinov2-large')\n",
        "#process=AutoImageProcessor.from_pretrained(\"facebook/dinov2-large\")\n",
        "\n",
        "#reeze parameter in dino\n",
        "for param in model_di.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "#model_h = ViTModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "#model_h = model_h.to(device)\n",
        "# Move the model to GPU if available\n",
        "\n",
        "# Define the custom dataset\n",
        "class ImageOneDDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_file, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get the list of JPEG files in the root directory\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
        "        # Load the labels and one-D data\n",
        "        self.labels = pd.read_excel(label_file).values\n",
        "        self.labels =  self.labels\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "         # Get the image file name\n",
        "        image_file = self.image_files[idx]\n",
        "\n",
        "        # Read the image from the file\n",
        "        image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
        "\n",
        "        # Apply transformations to the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get the label and one-D data\n",
        "        label =torch.tensor(self.labels).T[idx+1]/100\n",
        "\n",
        "\n",
        "\n",
        "        # Return the image, label, and one-D data\n",
        "        return image.float(), label.float()\n",
        "\n",
        "# Define the ResNet-MLP model\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet_18(nn.Module):\n",
        "\n",
        "    def __init__(self, image_channels, num_classes):\n",
        "\n",
        "        super(ResNet_18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        #resnet layers\n",
        "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
        "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
        "        self.layer2_3 = self.__make_layer(128, 128, stride=2)\n",
        "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
        "        self.layer3_4 = self.__make_layer(256, 256, stride=2)\n",
        "        self.layer4 = self.__make_layer(256, 64, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def __make_layer(self, in_channels, out_channels, stride):\n",
        "\n",
        "        identity_downsample = None\n",
        "        if stride != 1:\n",
        "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
        "\n",
        "        return nn.Sequential(\n",
        "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride),\n",
        "            Block(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def identity_downsample(self, in_channels, out_channels):\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        n=3\n",
        "        for i in range(n):\n",
        "            x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        for i in range(n):\n",
        "            x =  self.layer2_3(x)\n",
        "        x = self.layer3(x)\n",
        "        for i in range(n):\n",
        "             x = self.layer3_4(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class V_transform(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(V_transform, self).__init__()\n",
        "        model =VSSM()\n",
        "        self.model=model\n",
        "        #self.model = ResNet_18(3,700)\n",
        "\n",
        "        # Freeze the vit_b_32 parameters\n",
        "        #for param in self.model.parameters():\n",
        "         #   param.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(in_features=1000, out_features=500),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(in_features=500, out_features=50),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(in_features=50, out_features=20),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(in_features=20, out_features=6),\n",
        "          nn.Sigmoid())\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, images):\n",
        "        #x=process(images)\n",
        "        #x = self.model(**images).last_hidden_state[:, 0]  # Extract the CLS token\n",
        "        x=self.model(images)\n",
        "        x = self.fc(x)\n",
        "\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create the dataset\n",
        "dataset = ImageOneDDataset(image_dir='/content/drive/MyDrive/workspace/low_fine',\n",
        "                          label_file='/content/drive/MyDrive/workspace/low_fine/select_low_fine.xlsx',\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.Resize((1500, 2000)),\n",
        "                              transforms.ToTensor(),\n",
        "\n",
        "                          ]))\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = int(0.05 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = V_transform()\n",
        "model = model.to(device)\n",
        "model= nn.DataParallel(model,device_ids=[0])\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(60):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        #model= nn.DataParallel(model,device_ids=[0,1])\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}')\n",
        "# Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            # Move the inputs and targets to the GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            val_loss += loss_fn(outputs, targets).item()\n",
        "\n",
        "    # Print the validation loss\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f'Epoch: {epoch+1}, Validation Loss: {val_loss}')\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/workspace/model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "oo_a2WGy2ypU",
        "outputId": "c522f468-4068-4426-a48b-4b8b5ecf5aea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Epoch: 1, Batch: 1, Loss: 0.15824922919273376\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 375, 500, 96]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 187, 250, 192]) is not match even ===========\n",
            "Warning, x.shape torch.Size([1, 93, 125, 384]) is not match even ===========\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f3a2b349342b>\u001b[0m in \u001b[0;36m<cell line: 233>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m#model= nn.DataParallel(model,device_ids=[0,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDjFzXLB3gvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}