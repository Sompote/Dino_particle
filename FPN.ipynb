{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1m64K-1dTUMi5uuB0E9f1Z_FC77vR3sTb",
      "authorship_tag": "ABX9TyO1k3XZURjcW9oBs9Qw6Vpd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sompote/Dino_particle/blob/main/FPN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfk5LwgEorG7",
        "outputId": "a107cd35-635b-43b8-d30a-725de2ce1650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-fpn'...\n",
            "remote: Enumerating objects: 220, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 220 (delta 26), reused 27 (delta 10), pack-reused 175\u001b[K\n",
            "Receiving objects: 100% (220/220), 45.70 KiB | 15.23 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AdeelH/pytorch-fpn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ovSvdtEGQRwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pytorch-fpn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK7tVpCtpt2F",
        "outputId": "a90fe84c-528b-4bea-90a2-d0dc763ff05f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-fpn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, ViTModel,ViTFeatureExtractor,AutoModel\n",
        "from transformers import Dinov2Config, Dinov2Model\n",
        "import warnings\n",
        "from fpn.factory import make_fpn_resnet\n",
        "\n",
        "#torch.set_warn_always(True)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#config=Dinov2Config(image_size=1800)\n",
        "#model_di=Dinov2Model(config)\n",
        "\n",
        "#model_di = Dinov2Model.from_pretrained(\"facebook/dinov2-large\")\n",
        "\n",
        "#model_di = AutoModel.from_pretrained('facebook/dinov2-large')\n",
        "#process=AutoImageProcessor.from_pretrained(\"facebook/dinov2-large\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model_h = ViTModel.from_pretrained(\"facebook/dinov2-base\")\n",
        "#model_h = model_h.to(device)\n",
        "# Move the model to GPU if available\n",
        "\n",
        "# Define the custom dataset\n",
        "class ImageOneDDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_file, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get the list of JPEG files in the root directory\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
        "        # Load the labels and one-D data\n",
        "        self.labels = pd.read_excel(label_file).values\n",
        "        self.labels =  self.labels\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "         # Get the image file name\n",
        "        image_file = self.image_files[idx]\n",
        "\n",
        "        # Read the image from the file\n",
        "        image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
        "\n",
        "        # Apply transformations to the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get the label and one-D data\n",
        "        label =torch.tensor(self.labels).T[idx+1]/100\n",
        "\n",
        "\n",
        "\n",
        "        # Return the image, label, and one-D data\n",
        "        return image.float(), label.float()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FPN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes,img_h,img_w):\n",
        "\n",
        "        super(FPN, self).__init__()\n",
        "        self.img_h=img_h\n",
        "        self.img_w=img_w\n",
        "        self.num_classes=num_classes\n",
        "        self.model = make_fpn_resnet(\n",
        "          name='resnet18',\n",
        "          fpn_type='fpn',\n",
        "          pretrained=True,\n",
        "          num_classes=self.num_classes,\n",
        "          fpn_channels=256,\n",
        "          in_channels=3,\n",
        "          out_size=(self.img_h, self.img_w))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(in_features=(self.img_h*self.img_w*self.num_classes), out_features=1000),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(in_features=1000, out_features=50),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(in_features=50, out_features=20),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(in_features=20, out_features=6),\n",
        "          nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x=self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the dataset\n",
        "#resize image\n",
        "img_h=800\n",
        "img_w=600\n",
        "dataset = ImageOneDDataset(image_dir='/content/drive/MyDrive/workspace/low_fine',\n",
        "                          label_file='/content/drive/MyDrive/workspace/low_fine/select_low_fine.xlsx',\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.Resize((img_h, img_w)),\n",
        "                              transforms.ToTensor(),\n",
        "\n",
        "                          ]))\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = int(0.05 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = FPN(num_classes=3,img_h=img_h,img_w=img_w)\n",
        "model = model.to(device)\n",
        "#model= nn.DataParallel(model,device_ids=[0,1])\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(60):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        #model= nn.DataParallel(model,device_ids=[0])\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}')\n",
        "# Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            # Move the inputs and targets to the GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            val_loss += loss_fn(outputs, targets).item()\n",
        "\n",
        "    # Print the validation loss\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f'Epoch: {epoch+1}, Validation Loss: {val_loss}')\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/workspace/model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lmxLYue68fd",
        "outputId": "3ec23656-7ef6-4433-cb07-e06957649c1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 181MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 1, Loss: 0.16559654474258423\n",
            "Epoch: 1, Validation Loss: 0.07151041552424431\n",
            "Epoch: 2, Batch: 1, Loss: 0.0310337096452713\n",
            "Epoch: 2, Validation Loss: 0.03979512210935354\n",
            "Epoch: 3, Batch: 1, Loss: 0.02676735632121563\n",
            "Epoch: 3, Validation Loss: 0.03118635667487979\n",
            "Epoch: 4, Batch: 1, Loss: 0.0513351708650589\n",
            "Epoch: 4, Validation Loss: 0.026059717871248722\n",
            "Epoch: 5, Batch: 1, Loss: 0.012762892991304398\n",
            "Epoch: 5, Validation Loss: 0.02536962740123272\n",
            "Epoch: 6, Batch: 1, Loss: 0.01880600117146969\n",
            "Epoch: 6, Validation Loss: 0.024811766110360622\n",
            "Epoch: 7, Batch: 1, Loss: 0.01068948209285736\n",
            "Epoch: 7, Validation Loss: 0.0227939672768116\n",
            "Epoch: 8, Batch: 1, Loss: 0.0027727168053388596\n",
            "Epoch: 8, Validation Loss: 0.02339203073643148\n",
            "Epoch: 9, Batch: 1, Loss: 0.0014985942980274558\n",
            "Epoch: 9, Validation Loss: 0.02305656741373241\n",
            "Epoch: 10, Batch: 1, Loss: 0.0065908185206353664\n",
            "Epoch: 10, Validation Loss: 0.023902328219264746\n",
            "Epoch: 11, Batch: 1, Loss: 0.015236495062708855\n",
            "Epoch: 11, Validation Loss: 0.02221947442740202\n",
            "Epoch: 12, Batch: 1, Loss: 0.11877795308828354\n",
            "Epoch: 12, Validation Loss: 0.02268754946999252\n",
            "Epoch: 13, Batch: 1, Loss: 0.006260232534259558\n",
            "Epoch: 13, Validation Loss: 0.021901848260313272\n",
            "Epoch: 14, Batch: 1, Loss: 0.0065175751224160194\n",
            "Epoch: 14, Validation Loss: 0.023173709865659475\n",
            "Epoch: 15, Batch: 1, Loss: 0.011560055427253246\n",
            "Epoch: 15, Validation Loss: 0.02227501105517149\n",
            "Epoch: 16, Batch: 1, Loss: 0.009110303595662117\n",
            "Epoch: 16, Validation Loss: 0.022715965751558542\n",
            "Epoch: 17, Batch: 1, Loss: 0.05038365721702576\n",
            "Epoch: 17, Validation Loss: 0.02335000690072775\n",
            "Epoch: 18, Batch: 1, Loss: 0.027474908158183098\n",
            "Epoch: 18, Validation Loss: 0.02220674231648445\n",
            "Epoch: 19, Batch: 1, Loss: 0.02744276635348797\n",
            "Epoch: 19, Validation Loss: 0.022556323325261474\n",
            "Epoch: 20, Batch: 1, Loss: 0.05446423217654228\n",
            "Epoch: 20, Validation Loss: 0.02206926653161645\n",
            "Epoch: 21, Batch: 1, Loss: 0.008433671668171883\n",
            "Epoch: 21, Validation Loss: 0.022016151808202267\n",
            "Epoch: 22, Batch: 1, Loss: 0.0524301640689373\n",
            "Epoch: 22, Validation Loss: 0.023657770827412605\n",
            "Epoch: 23, Batch: 1, Loss: 0.027345716953277588\n",
            "Epoch: 23, Validation Loss: 0.022536449134349823\n",
            "Epoch: 24, Batch: 1, Loss: 0.018724270164966583\n",
            "Epoch: 24, Validation Loss: 0.022223784122616053\n",
            "Epoch: 25, Batch: 1, Loss: 0.0022474846336990595\n",
            "Epoch: 25, Validation Loss: 0.02209283923730254\n",
            "Epoch: 26, Batch: 1, Loss: 0.029333844780921936\n",
            "Epoch: 26, Validation Loss: 0.023140146164223552\n",
            "Epoch: 27, Batch: 1, Loss: 0.005574491806328297\n",
            "Epoch: 27, Validation Loss: 0.022977307438850403\n",
            "Epoch: 28, Batch: 1, Loss: 0.028162455186247826\n",
            "Epoch: 28, Validation Loss: 0.022485909517854452\n",
            "Epoch: 29, Batch: 1, Loss: 0.0010100059444084764\n",
            "Epoch: 29, Validation Loss: 0.022601603530347347\n",
            "Epoch: 30, Batch: 1, Loss: 0.0077776662074029446\n",
            "Epoch: 30, Validation Loss: 0.02180673833936453\n",
            "Epoch: 31, Batch: 1, Loss: 0.008208004757761955\n",
            "Epoch: 31, Validation Loss: 0.022856364492326975\n",
            "Epoch: 32, Batch: 1, Loss: 0.013191577047109604\n",
            "Epoch: 32, Validation Loss: 0.02290113689377904\n",
            "Epoch: 33, Batch: 1, Loss: 0.03291281312704086\n",
            "Epoch: 33, Validation Loss: 0.022028525127097964\n",
            "Epoch: 34, Batch: 1, Loss: 0.010774297639727592\n",
            "Epoch: 34, Validation Loss: 0.022497962694615126\n",
            "Epoch: 35, Batch: 1, Loss: 0.010925671085715294\n",
            "Epoch: 35, Validation Loss: 0.023644451750442386\n",
            "Epoch: 36, Batch: 1, Loss: 0.008177532814443111\n",
            "Epoch: 36, Validation Loss: 0.02289304183796048\n",
            "Epoch: 37, Batch: 1, Loss: 0.03657557815313339\n",
            "Epoch: 37, Validation Loss: 0.02276720991358161\n",
            "Epoch: 38, Batch: 1, Loss: 0.009519940242171288\n",
            "Epoch: 38, Validation Loss: 0.022934114560484886\n",
            "Epoch: 39, Batch: 1, Loss: 0.009550127200782299\n",
            "Epoch: 39, Validation Loss: 0.022180662024766207\n",
            "Epoch: 40, Batch: 1, Loss: 0.027751123532652855\n",
            "Epoch: 40, Validation Loss: 0.02332301950082183\n",
            "Epoch: 41, Batch: 1, Loss: 0.001446116715669632\n",
            "Epoch: 41, Validation Loss: 0.022387330885976553\n",
            "Epoch: 42, Batch: 1, Loss: 0.005755235906690359\n",
            "Epoch: 42, Validation Loss: 0.02297918638214469\n",
            "Epoch: 43, Batch: 1, Loss: 0.007063515018671751\n",
            "Epoch: 43, Validation Loss: 0.022410640958696604\n",
            "Epoch: 44, Batch: 1, Loss: 0.0057309772819280624\n",
            "Epoch: 44, Validation Loss: 0.022374541964381933\n",
            "Epoch: 45, Batch: 1, Loss: 0.001838186988607049\n",
            "Epoch: 45, Validation Loss: 0.023007775424048305\n",
            "Epoch: 46, Batch: 1, Loss: 0.004637818783521652\n",
            "Epoch: 46, Validation Loss: 0.022603157442063093\n",
            "Epoch: 47, Batch: 1, Loss: 0.02812439762055874\n",
            "Epoch: 47, Validation Loss: 0.023235668195411563\n",
            "Epoch: 48, Batch: 1, Loss: 0.05196646600961685\n",
            "Epoch: 48, Validation Loss: 0.022893413435667753\n",
            "Epoch: 49, Batch: 1, Loss: 0.019077425822615623\n",
            "Epoch: 49, Validation Loss: 0.02273434540256858\n",
            "Epoch: 50, Batch: 1, Loss: 0.015531077049672604\n",
            "Epoch: 50, Validation Loss: 0.023850455414503813\n",
            "Epoch: 51, Batch: 1, Loss: 0.006584891118109226\n",
            "Epoch: 51, Validation Loss: 0.0230144951492548\n",
            "Epoch: 52, Batch: 1, Loss: 0.036310866475105286\n",
            "Epoch: 52, Validation Loss: 0.022333512781187892\n",
            "Epoch: 53, Batch: 1, Loss: 0.029191546142101288\n",
            "Epoch: 53, Validation Loss: 0.022688030963763595\n",
            "Epoch: 54, Batch: 1, Loss: 0.018235817551612854\n",
            "Epoch: 54, Validation Loss: 0.023175574373453856\n",
            "Epoch: 55, Batch: 1, Loss: 0.004822758492082357\n",
            "Epoch: 55, Validation Loss: 0.022026349557563663\n",
            "Epoch: 56, Batch: 1, Loss: 0.005969881545752287\n",
            "Epoch: 56, Validation Loss: 0.02371087367646396\n",
            "Epoch: 57, Batch: 1, Loss: 0.001998179592192173\n",
            "Epoch: 57, Validation Loss: 0.02291031274944544\n",
            "Epoch: 58, Batch: 1, Loss: 0.011445030570030212\n",
            "Epoch: 58, Validation Loss: 0.02256243070587516\n",
            "Epoch: 59, Batch: 1, Loss: 0.005419082473963499\n",
            "Epoch: 59, Validation Loss: 0.023339599138125777\n",
            "Epoch: 60, Batch: 1, Loss: 0.0355764739215374\n",
            "Epoch: 60, Validation Loss: 0.021944409934803843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oo1RU4bt-QPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mape(y_true, y_pred):\n",
        "    # Avoid division by zero\n",
        "    y_true = torch.clamp(y_true, min=1e-8)\n",
        "    # Compute the absolute percentage error\n",
        "    ape = torch.abs((y_true - y_pred) / y_true)*100\n",
        "    # Return the mean over all predictions\n",
        "    return torch.mean(ape)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_loss = 0.0\n",
        "    test_mape = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        # Move the inputs and targets to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        test_loss += criterion(outputs, targets).item()\n",
        "        # Compute the MAPE\n",
        "        test_mape += mape(targets, outputs).item()\n",
        "\n",
        "    # Print the test loss and MAPE\n",
        "    test_loss /= len(test_loader)\n",
        "    test_mape /= len(test_loader)\n",
        "    print(f'Test Loss: {test_loss}')\n",
        "    print(f'Test MAPE: {test_mape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl0xIS58-LEd",
        "outputId": "c3d06e22-7d26-426d-a9f2-19f65d6c634c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.02373806480318308\n",
            "Test MAPE: 22.36635971069336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgIANbUy8ZM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}