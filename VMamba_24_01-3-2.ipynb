{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gv5dU5kj1MTB",
    "outputId": "5c380842-0df9-4769-b29c-5b5a080c7469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 23.0.1 from /opt/conda/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting timm==0.4.12\n",
      "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.4.12) (0.15.2)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from timm==0.4.12) (2.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (1.24.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.4.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (7.4.3)\n",
      "Requirement already satisfied: chardet in /opt/conda/lib/python3.10/site-packages (4.0.0)\n",
      "Collecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest) (23.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest) (2.0.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs) (6.0)\n",
      "Installing collected packages: yacs, termcolor\n",
      "Successfully installed termcolor-2.4.0 yacs-0.1.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting submitit\n",
      "  Downloading submitit-1.5.1-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.7.4.2 in /opt/conda/lib/python3.10/site-packages (from submitit) (4.5.0)\n",
      "Collecting cloudpickle>=1.2.1\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (23.0)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (1.24.3)\n",
      "Installing collected packages: protobuf, cloudpickle, tensorboardX, submitit\n",
      "Successfully installed cloudpickle-3.0.0 protobuf-4.25.2 submitit-1.5.1 tensorboardX-2.6.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (2.0.0)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0) (3.9.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0) (2.0.1)\n",
      "Collecting lit\n",
      "  Downloading lit-17.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->triton==2.0.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->triton==2.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->triton==2.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->triton==2.0.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->triton==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->triton==2.0.0) (1.3.0)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=cf066947575d7c361638c044e303cc58b11461628020f647c0b399baa8900001\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake\n",
      "Successfully installed cmake-3.28.1 lit-17.0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting causal_conv1d==1.0.0\n",
      "  Downloading causal_conv1d-1.0.0.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from causal_conv1d==1.0.0) (2.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from causal_conv1d==1.0.0) (23.0)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.0.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.0.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->causal_conv1d==1.0.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->causal_conv1d==1.0.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->causal_conv1d==1.0.0) (1.3.0)\n",
      "Building wheels for collected packages: causal_conv1d\n",
      "  Building wheel for causal_conv1d (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for causal_conv1d: filename=causal_conv1d-1.0.0-cp310-cp310-linux_x86_64.whl size=9145277 sha256=476b13556bd7068d8d0ca49a5073677b8ee3c5f7a92d6be3159ff8b3b9fd063e\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/48/f5/eb0c6d6d8e00131eaa57927b537a23832b37e2f01b801d9c5d\n",
      "Successfully built causal_conv1d\n",
      "Installing collected packages: ninja, causal_conv1d\n",
      "Successfully installed causal_conv1d-1.0.0 ninja-1.11.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting mamba_ssm==1.0.1\n",
      "  Downloading mamba_ssm-1.0.1.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.0.1) (2.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.0.1) (23.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.0.1) (1.11.1.1)\n",
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.0.1) (2.0.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: causal_conv1d in /opt/conda/lib/python3.10/site-packages (from mamba_ssm==1.0.1) (1.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.0.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.0.1) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->mamba_ssm==1.0.1) (3.1.2)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.0.1) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.0.1) (1.24.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.0.1) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->mamba_ssm==1.0.1) (2.31.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton->mamba_ssm==1.0.1) (3.28.1)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton->mamba_ssm==1.0.1) (17.0.6)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->mamba_ssm==1.0.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.0.1) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.0.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba_ssm==1.0.1) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->mamba_ssm==1.0.1) (1.3.0)\n",
      "Building wheels for collected packages: mamba_ssm\n",
      "  Building wheel for mamba_ssm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mamba_ssm: filename=mamba_ssm-1.0.1-cp310-cp310-linux_x86_64.whl size=146630099 sha256=b2131da71619353377bee35d0e2359f7fc82d71fdf91ab8fb487f56d27152182\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/cf/65/cc589985f9689241fe2c154ce1c60738f58a24e76ce474cc20\n",
      "Successfully built mamba_ssm\n",
      "Installing collected packages: safetensors, regex, fsspec, einops, huggingface-hub, tokenizers, transformers, mamba_ssm\n",
      "Successfully installed einops-0.7.0 fsspec-2023.12.2 huggingface-hub-0.20.3 mamba_ssm-1.0.1 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.1 transformers-4.37.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -V\n",
    "#!pip install torch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# We use py110 cu117 torch113\n",
    "!pip install packaging\n",
    "!pip install timm==0.4.12\n",
    "!pip install pytest chardet yacs termcolor\n",
    "!pip install submitit tensorboardX\n",
    "!pip install triton==2.0.0\n",
    "!pip install causal_conv1d==1.0.0  # causal_conv1d-1.0.0+cu118torch1.13cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
    "!pip install mamba_ssm==1.0.1  # mamba_ssm-1.0.1+cu118torch1.13cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8siAngBF2hFE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Optional, Callable\n",
    "import warnings\n",
    "\n",
    "#torch.set_warn_always(True)\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "try:\n",
    "    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# an alternative for mamba_ssm (in which causal_conv1d is needed)\n",
    "try:\n",
    "    from selective_scan import selective_scan_fn as selective_scan_fn_v1\n",
    "    from selective_scan import selective_scan_ref as selective_scan_ref_v1\n",
    "except:\n",
    "    pass\n",
    "\n",
    "DropPath.__repr__ = lambda self: f\"timm.DropPath({self.drop_prob})\"\n",
    "\n",
    "\n",
    "def flops_selective_scan_ref(B=1, L=256, D=768, N=16, with_D=True, with_Z=False, with_Group=True, with_complex=False):\n",
    "    \"\"\"\n",
    "    u: r(B D L)\n",
    "    delta: r(B D L)\n",
    "    A: r(D N)\n",
    "    B: r(B N L)\n",
    "    C: r(B N L)\n",
    "    D: r(D)\n",
    "    z: r(B D L)\n",
    "    delta_bias: r(D), fp32\n",
    "\n",
    "    ignores:\n",
    "        [.float(), +, .softplus, .shape, new_zeros, repeat, stack, to(dtype), silu]\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # fvcore.nn.jit_handles\n",
    "    def get_flops_einsum(input_shapes, equation):\n",
    "        np_arrs = [np.zeros(s) for s in input_shapes]\n",
    "        optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n",
    "        for line in optim.split(\"\\n\"):\n",
    "            if \"optimized flop\" in line.lower():\n",
    "                # divided by 2 because we count MAC (multiply-add counted as one flop)\n",
    "                flop = float(np.floor(float(line.split(\":\")[-1]) / 2))\n",
    "                return flop\n",
    "\n",
    "\n",
    "    assert not with_complex\n",
    "\n",
    "    flops = 0 # below code flops = 0\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        dtype_in = u.dtype\n",
    "        u = u.float()\n",
    "        delta = delta.float()\n",
    "        if delta_bias is not None:\n",
    "            delta = delta + delta_bias[..., None].float()\n",
    "        if delta_softplus:\n",
    "            delta = F.softplus(delta)\n",
    "        batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
    "        is_variable_B = B.dim() >= 3\n",
    "        is_variable_C = C.dim() >= 3\n",
    "        if A.is_complex():\n",
    "            if is_variable_B:\n",
    "                B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
    "            if is_variable_C:\n",
    "                C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
    "        else:\n",
    "            B = B.float()\n",
    "            C = C.float()\n",
    "        x = A.new_zeros((batch, dim, dstate))\n",
    "        ys = []\n",
    "        \"\"\"\n",
    "\n",
    "    flops += get_flops_einsum([[B, D, L], [D, N]], \"bdl,dn->bdln\")\n",
    "    if with_Group:\n",
    "        flops += get_flops_einsum([[B, D, L], [B, N, L], [B, D, L]], \"bdl,bnl,bdl->bdln\")\n",
    "    else:\n",
    "        flops += get_flops_einsum([[B, D, L], [B, D, N, L], [B, D, L]], \"bdl,bdnl,bdl->bdln\")\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "        if not is_variable_B:\n",
    "            deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
    "        else:\n",
    "            if B.dim() == 3:\n",
    "                deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
    "            else:\n",
    "                B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
    "                deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
    "        if is_variable_C and C.dim() == 4:\n",
    "            C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
    "        last_state = None\n",
    "        \"\"\"\n",
    "\n",
    "    in_for_flops = B * D * N\n",
    "    if with_Group:\n",
    "        in_for_flops += get_flops_einsum([[B, D, N], [B, D, N]], \"bdn,bdn->bd\")\n",
    "    else:\n",
    "        in_for_flops += get_flops_einsum([[B, D, N], [B, N]], \"bdn,bn->bd\")\n",
    "    flops += L * in_for_flops\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        for i in range(u.shape[2]):\n",
    "            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
    "            if not is_variable_C:\n",
    "                y = torch.einsum('bdn,dn->bd', x, C)\n",
    "            else:\n",
    "                if C.dim() == 3:\n",
    "                    y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
    "                else:\n",
    "                    y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
    "            if i == u.shape[2] - 1:\n",
    "                last_state = x\n",
    "            if y.is_complex():\n",
    "                y = y.real * 2\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=2) # (batch dim L)\n",
    "        \"\"\"\n",
    "\n",
    "    if with_D:\n",
    "        flops += B * D * L\n",
    "    if with_Z:\n",
    "        flops += B * D * L\n",
    "    if False:\n",
    "        ...\n",
    "        \"\"\"\n",
    "        out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
    "        if z is not None:\n",
    "            out = out * F.silu(z)\n",
    "        out = out.to(dtype=dtype_in)\n",
    "        \"\"\"\n",
    "\n",
    "    return flops\n",
    "\n",
    "\n",
    "class PatchEmbed2D(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, **kwargs):\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).permute(0, 2, 3, 1)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging2D(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        SHAPE_FIX = [-1, -1]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
    "            SHAPE_FIX[0] = H // 2\n",
    "            SHAPE_FIX[1] = W // 2\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "\n",
    "        if SHAPE_FIX[0] > 0:\n",
    "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, H//2, W//2, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SS2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        # d_state=\"auto\", # 20240109\n",
    "        d_conv=3,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        dropout=0.,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        # self.d_state = math.ceil(self.d_model / 6) if d_state == \"auto\" else d_model # 20240109\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            groups=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            padding=(d_conv - 1) // 2,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = (\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs),\n",
    "        )\n",
    "        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)\n",
    "        del self.x_proj\n",
    "\n",
    "        self.dt_projs = (\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),\n",
    "        )\n",
    "        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)\n",
    "        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)\n",
    "        del self.dt_projs\n",
    "\n",
    "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "\n",
    "        # self.selective_scan = selective_scan_fn\n",
    "        self.forward_core = self.forward_corev0\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
    "\n",
    "    @staticmethod\n",
    "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):\n",
    "        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        dt_proj.bias._no_reinit = True\n",
    "\n",
    "        return dt_proj\n",
    "\n",
    "    @staticmethod\n",
    "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        if copies > 1:\n",
    "            A_log = repeat(A_log, \"d n -> r d n\", r=copies)\n",
    "            if merge:\n",
    "                A_log = A_log.flatten(0, 1)\n",
    "        A_log = nn.Parameter(A_log)\n",
    "        A_log._no_weight_decay = True\n",
    "        return A_log\n",
    "\n",
    "    @staticmethod\n",
    "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
    "        # D \"skip\" parameter\n",
    "        D = torch.ones(d_inner, device=device)\n",
    "        if copies > 1:\n",
    "            D = repeat(D, \"n1 -> r n1\", r=copies)\n",
    "            if merge:\n",
    "                D = D.flatten(0, 1)\n",
    "        D = nn.Parameter(D)  # Keep in fp32\n",
    "        D._no_weight_decay = True\n",
    "        return D\n",
    "\n",
    "    def forward_corev0(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
    "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1) # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts,\n",
    "            As, Bs, Cs, Ds, z=None,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=False,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    # an alternative to forward_corev1\n",
    "    def forward_corev1(self, x: torch.Tensor):\n",
    "        self.selective_scan = selective_scan_fn_v1\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)\n",
    "        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)\n",
    "\n",
    "        x_dbl = torch.einsum(\"b k d l, k c d -> b k c l\", xs.view(B, K, -1, L), self.x_proj_weight)\n",
    "        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)\n",
    "        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)\n",
    "        dts = torch.einsum(\"b k r l, k d r -> b k d l\", dts.view(B, K, -1, L), self.dt_projs_weight)\n",
    "        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)\n",
    "\n",
    "        xs = xs.float().view(B, -1, L) # (b, k * d, l)\n",
    "        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)\n",
    "        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)\n",
    "        Ds = self.Ds.float().view(-1) # (k * d)\n",
    "        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts,\n",
    "            As, Bs, Cs, Ds,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "        ).view(B, K, -1, L)\n",
    "        assert out_y.dtype == torch.float\n",
    "\n",
    "        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)\n",
    "        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        xz = self.in_proj(x)\n",
    "        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = self.act(self.conv2d(x)) # (b, d, h, w)\n",
    "        y1, y2, y3, y4 = self.forward_core(x)\n",
    "        assert y1.dtype == torch.float32\n",
    "        y = y1 + y2 + y3 + y4\n",
    "        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)\n",
    "        y = self.out_norm(y)\n",
    "        y = y * F.silu(z)\n",
    "        out = self.out_proj(y)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VSSBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 0,\n",
    "        drop_path: float = 0,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        attn_drop_rate: float = 0,\n",
    "        d_state: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = SS2D(d_model=hidden_dim, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        x = input + self.drop_path(self.self_attention(self.ln_1(input)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        attn_drop=0.,\n",
    "        drop_path=0.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "        d_state=16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VSSBlock(\n",
    "                hidden_dim=dim,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer,\n",
    "                attn_drop_rate=attn_drop,\n",
    "                d_state=d_state,\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "\n",
    "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
    "            def _init_weights(module: nn.Module):\n",
    "                for name, p in module.named_parameters():\n",
    "                    if name in [\"out_proj.weight\"]:\n",
    "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
    "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "            self.apply(_init_weights)\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSM(nn.Module):\n",
    "    def __init__(self, patch_size=4, in_chans=3, num_classes=1000, depths=[2, 2, 9, 2],\n",
    "                 dims=[96, 192, 384, 768], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        if isinstance(dims, int):\n",
    "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
    "        self.embed_dim = dims[0]\n",
    "        self.num_features = dims[-1]\n",
    "        self.dims = dims\n",
    "\n",
    "        self.patch_embed = PatchEmbed2D(patch_size=patch_size, in_chans=in_chans, embed_dim=self.embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None)\n",
    "\n",
    "        # WASTED absolute position embedding ======================\n",
    "        self.ape = False\n",
    "        # self.ape = False\n",
    "        # drop_rate = 0.0\n",
    "        if self.ape:\n",
    "            self.patches_resolution = self.patch_embed.patches_resolution\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, *self.patches_resolution, self.embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = VSSLayer(\n",
    "                dim=dims[i_layer],\n",
    "                depth=depths[i_layer],\n",
    "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging2D if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m: nn.Module):\n",
    "        \"\"\"\n",
    "        out_proj.weight which is previously initilized in VSSBlock, would be cleared in nn.Linear\n",
    "        no fc.weight found in the any of the model parameters\n",
    "        no nn.Embedding found in the any of the model parameters\n",
    "        so the thing is, VSSBlock initialization is useless\n",
    "\n",
    "        Conv2D is not intialized !!!\n",
    "        \"\"\"\n",
    "        # print(m, getattr(getattr(m, \"weight\", nn.Identity()), \"INIT\", None), isinstance(m, nn.Linear), \"======================\")\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = torch.flatten(x, 1, 2) # B H W C -> B L C\n",
    "        \n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFkJsXpkBiqJ",
    "outputId": "76acafec-0956-471a-f8fa-58678217a177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.0 tzdata-2023.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "id": "oo_a2WGy2ypU",
    "outputId": "c522f468-4068-4426-a48b-4b8b5ecf5aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 1, Batch: 1, Loss: 0.1657991111278534\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 1, Validation Loss: 0.04207548871636391\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 2, Batch: 1, Loss: 0.0611741840839386\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 2, Validation Loss: 0.017015731893479824\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 3, Batch: 1, Loss: 0.04941170662641525\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 3, Validation Loss: 0.00883225305005908\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 4, Batch: 1, Loss: 0.01659407652914524\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 4, Validation Loss: 0.005448100622743368\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 5, Batch: 1, Loss: 0.006003021262586117\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 5, Validation Loss: 0.0048307920806109905\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 6, Batch: 1, Loss: 0.009865905158221722\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 6, Validation Loss: 0.004438564646989107\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Epoch: 7, Batch: 1, Loss: 0.00972725823521614\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([1, 225, 312, 192]) is not match even ===========\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ViTModel,ViTFeatureExtractor,AutoModel\n",
    "from transformers import Dinov2Config, Dinov2Model\n",
    "import warnings\n",
    "\n",
    "#torch.set_warn_always(True)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "config=Dinov2Config(image_size=1800)\n",
    "model_di=Dinov2Model(config)\n",
    "\n",
    "#model_di = Dinov2Model.from_pretrained(\"facebook/dinov2-large\")\n",
    "\n",
    "#model_di = AutoModel.from_pretrained('facebook/dinov2-large')\n",
    "#process=AutoImageProcessor.from_pretrained(\"facebook/dinov2-large\")\n",
    "\n",
    "#reeze parameter in dino\n",
    "for param in model_di.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "#model_h = ViTModel.from_pretrained(\"facebook/dinov2-base\")\n",
    "#model_h = model_h.to(device)\n",
    "# Move the model to GPU if available\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageOneDDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get the list of JPEG files in the root directory\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
    "        # Load the labels and one-D data\n",
    "        self.labels = pd.read_excel(label_file).values\n",
    "        self.labels =  self.labels\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         # Get the image file name\n",
    "        image_file = self.image_files[idx]\n",
    "\n",
    "        # Read the image from the file\n",
    "        image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get the label and one-D data\n",
    "        label =torch.tensor(self.labels).T[idx+1]/100\n",
    "\n",
    "\n",
    "\n",
    "        # Return the image, label, and one-D data\n",
    "        return image.float(), label.float()\n",
    "\n",
    "# Define the ResNet-MLP model\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet_18(nn.Module):\n",
    "\n",
    "    def __init__(self, image_channels, num_classes):\n",
    "\n",
    "        super(ResNet_18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer2_3 = self.__make_layer(128, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
    "        self.layer3_4 = self.__make_layer(256, 256, stride=2)\n",
    "        self.layer4 = self.__make_layer(256, 64, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def __make_layer(self, in_channels, out_channels, stride):\n",
    "\n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "\n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride),\n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        n=3\n",
    "        for i in range(n):\n",
    "            x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        for i in range(n):\n",
    "            x =  self.layer2_3(x)\n",
    "        x = self.layer3(x)\n",
    "        for i in range(n):\n",
    "             x = self.layer3_4(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class V_transform(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(V_transform, self).__init__()\n",
    "        model =VSSM()\n",
    "        self.model=model\n",
    "        #self.model = ResNet_18(3,700)\n",
    "\n",
    "        # Freeze the vit_b_32 parameters\n",
    "        #for param in self.model.parameters():\n",
    "         #   param.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "          nn.Linear(in_features=1000, out_features=500),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(in_features=500, out_features=50),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(in_features=50, out_features=20),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(in_features=20, out_features=6),\n",
    "          nn.Sigmoid())\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        #x=process(images)\n",
    "        #x = self.model(**images).last_hidden_state[:, 0]  # Extract the CLS token\n",
    "        x=self.model(images)\n",
    "        x = self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageOneDDataset(image_dir='/workspace/low_fine',\n",
    "                          label_file='/workspace/low_fine/select_low_fine.xlsx',\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.Resize((1800, 2500)),\n",
    "                              transforms.ToTensor(),\n",
    "\n",
    "                          ]))\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = int(0.05 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = V_transform()\n",
    "model = model.to(device)\n",
    "#model= nn.DataParallel(model,device_ids=[0,1])\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(60):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        model= nn.DataParallel(model,device_ids=[0])\n",
    "\n",
    "        images = images.to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}')\n",
    "# Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # Move the inputs and targets to the GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            val_loss += loss_fn(outputs, targets).item()\n",
    "\n",
    "    # Print the validation loss\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch: {epoch+1}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/workspace/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "EDjFzXLB3gvD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, x.shape torch.Size([2, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([2, 225, 312, 192]) is not match even ===========\n",
      "Warning, x.shape torch.Size([2, 450, 625, 96]) is not match even ===========\n",
      "Warning, x.shape torch.Size([2, 225, 312, 192]) is not match even ===========\n",
      "Test Loss: 0.0932067483663559\n",
      "Test MAPE: 349.97398376464844\n"
     ]
    }
   ],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    # Avoid division by zero\n",
    "    y_true = torch.clamp(y_true, min=1e-8)\n",
    "    # Compute the absolute percentage error\n",
    "    ape = torch.abs((y_true - y_pred) / y_true)*100\n",
    "    # Return the mean over all predictions\n",
    "    return torch.mean(ape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    test_mape = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        # Move the inputs and targets to the GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        test_loss += criterion(outputs, targets).item()\n",
    "        # Compute the MAPE\n",
    "        test_mape += mape(targets, outputs).item()\n",
    "\n",
    "    # Print the test loss and MAPE\n",
    "    test_loss /= len(test_loader)\n",
    "    test_mape /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test MAPE: {test_mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
